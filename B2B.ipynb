{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\olga_bassant_roadmap\\B2B_project\\-TalentForge\\myenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Load the Phi-2 model and tokenizer\n",
    "# model_name = \"microsoft/phi-2\"  \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# # Load LLaMA 2 model using Ollama\n",
    "# llm = Ollama(model=\"llama2\", temperature=0.7)\n",
    "\n",
    "# # Example usage\n",
    "# prompt = \"Explain quantum computing in simple terms.\"\n",
    "# response = llm(prompt)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import re\n",
    "\n",
    "# # Load spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Function to extract information from CV text\n",
    "# def extract_cv_info(cv_text):\n",
    "#     # Initialize a dictionary to store the extracted information\n",
    "#     cv_data = {\n",
    "#         \"name\": None,\n",
    "#         \"contact\": {\"email\": None, \"phone\": None},\n",
    "#         \"education\": [],\n",
    "#         \"experience\": [],\n",
    "#         \"skills\": []\n",
    "#     }\n",
    "\n",
    "#     # Extract name using spaCy's NER\n",
    "#     doc = nlp(cv_text)\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ == \"PERSON\":  # Assuming the first PERSON entity is the name\n",
    "#             cv_data[\"name\"] = ent.text\n",
    "#             break\n",
    "\n",
    "#     # Extract email and phone using regex\n",
    "#     email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "#     phone_pattern = r\"\\+?\\d[\\d -]{8,12}\\d\"\n",
    "#     cv_data[\"contact\"][\"email\"] = re.findall(email_pattern, cv_text)[0]\n",
    "#     cv_data[\"contact\"][\"phone\"] = re.findall(phone_pattern, cv_text)[0]\n",
    "\n",
    "#     # Extract education\n",
    "#     education_section = re.search(r\"Education:(.*?)(Experience:|Skills:|$)\", cv_text, re.DOTALL)\n",
    "#     if education_section:\n",
    "#         education_lines = education_section.group(1).strip().split(\"\\n\")\n",
    "#         cv_data[\"education\"] = [line.strip() for line in education_lines if line.strip()]\n",
    "\n",
    "#     # Extract experience\n",
    "#     experience_section = re.search(r\"Experience:(.*?)(Skills:|$)\", cv_text, re.DOTALL)\n",
    "#     if experience_section:\n",
    "#         experience_lines = experience_section.group(1).strip().split(\"\\n\")\n",
    "#         cv_data[\"experience\"] = [line.strip() for line in experience_lines if line.strip()]\n",
    "\n",
    "#     # Extract skills\n",
    "#     skills_section = re.search(r\"Skills:(.*?)$\", cv_text, re.DOTALL)\n",
    "#     if skills_section:\n",
    "#         skills_lines = skills_section.group(1).strip().split(\"\\n\")\n",
    "#         cv_data[\"skills\"] = [line.strip() for line in skills_lines if line.strip()]\n",
    "\n",
    "#     return cv_data\n",
    "\n",
    "# # Example CV text\n",
    "# cv_text = \"\"\"\n",
    "# Name: John Doe\n",
    "# Email: johndoe@example.com\n",
    "# Phone: +1234567890\n",
    "\n",
    "# Education:\n",
    "# - Bachelor of Science in Computer Science, XYZ University, 2015-2019\n",
    "# - Master of Science in Data Science, ABC University, 2019-2021\n",
    "\n",
    "# Experience:\n",
    "# - Software Engineer at TechCorp, 2021-Present\n",
    "# - Data Analyst at DataWorks, 2019-2021\n",
    "\n",
    "# Skills:\n",
    "# - Python\n",
    "# - Machine Learning\n",
    "# - SQL\n",
    "# - Data Analysis\n",
    "# \"\"\"\n",
    "\n",
    "# # Extract information from the CV\n",
    "# cv_data = extract_cv_info(cv_text)\n",
    "# print(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CV from text file\n",
    "with open(\"cv1.txt\", \"r\") as file:\n",
    "    cv_text = file.read()\n",
    "\n",
    "# Read job description from text file\n",
    "with open(\"Job Description1.txt\", \"r\") as file:\n",
    "    job_description = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted CV Data: {'name': 'John Doe\\nEmail', 'contact': {'email': 'johndoe@example.com', 'phone': '+1234567890'}, 'education': ['- Bachelor of Science in Computer Science, XYZ University, 2015-2019', '- Master of Science in Data Science, ABC University, 2019-2021'], 'experience': ['- Software Engineer at TechCorp, 2021-Present', '- Data Analyst at DataWorks, 2019-2021'], 'skills': ['- Python', '- Machine Learning', '- SQL', '- Data Analysis']}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract information from CV text\n",
    "def extract_cv_info(cv_text):\n",
    "    cv_data = {\n",
    "        \"name\": None,\n",
    "        \"contact\": {\"email\": None, \"phone\": None},\n",
    "        \"education\": [],\n",
    "        \"experience\": [],\n",
    "        \"skills\": []\n",
    "    }\n",
    "\n",
    "    # Extract name using spaCy's NER\n",
    "    doc = nlp(cv_text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            cv_data[\"name\"] = ent.text\n",
    "            break\n",
    "\n",
    "    # Extract email and phone using regex\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    phone_pattern = r\"\\+?\\d[\\d -]{8,12}\\d\"\n",
    "    cv_data[\"contact\"][\"email\"] = re.findall(email_pattern, cv_text)[0]\n",
    "    cv_data[\"contact\"][\"phone\"] = re.findall(phone_pattern, cv_text)[0]\n",
    "\n",
    "    # Extract education\n",
    "    education_section = re.search(r\"Education:(.*?)(Experience:|Skills:|$)\", cv_text, re.DOTALL)\n",
    "    if education_section:\n",
    "        education_lines = education_section.group(1).strip().split(\"\\n\")\n",
    "        cv_data[\"education\"] = [line.strip() for line in education_lines if line.strip()]\n",
    "\n",
    "    # Extract experience\n",
    "    experience_section = re.search(r\"Experience:(.*?)(Skills:|$)\", cv_text, re.DOTALL)\n",
    "    if experience_section:\n",
    "        experience_lines = experience_section.group(1).strip().split(\"\\n\")\n",
    "        cv_data[\"experience\"] = [line.strip() for line in experience_lines if line.strip()]\n",
    "\n",
    "    # Extract skills\n",
    "    skills_section = re.search(r\"Skills:(.*?)$\", cv_text, re.DOTALL)\n",
    "    if skills_section:\n",
    "        skills_lines = skills_section.group(1).strip().split(\"\\n\")\n",
    "        cv_data[\"skills\"] = [line.strip() for line in skills_lines if line.strip()]\n",
    "\n",
    "    return cv_data\n",
    "\n",
    "# Extract information from the CV\n",
    "cv_data = extract_cv_info(cv_text)\n",
    "print(\"Extracted CV Data:\", cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_12140\\2269350488.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\", temperature=0.7)\n",
      "C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_12140\\2269350488.py:32: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  recommendation_letter = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recommendation Letter:\n",
      " Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my interest in the Data Scientist position at TechCorp. As a skilled computer scientist with experience in building predictive models and working with large datasets, I believe I would be an excellent fit for this role.\n",
      "\n",
      "My educational background includes a Bachelor of Science in Computer Science from XYZ University, as well as a Master of Science in Data Science from ABC University. My education has provided me with a solid foundation in programming languages such as Python, as well as machine learning and data analysis techniques.\n",
      "\n",
      "In my current position at TechCorp, I have gained valuable experience working as a Software Engineer. I have developed a strong understanding of software development life cycles, including design, development, testing, and deployment. Additionally, I have honed my skills in Python, SQL, and machine learning, which have allowed me to effectively analyze and interpret large datasets.\n",
      "\n",
      "My experience at DataWorks as a Data Analyst has further enhanced my skills in data analysis and interpretation. I have worked with various stakeholders to gather requirements, design databases, and develop reports and visualizations that communicate complex data insights to both technical and non-technical audiences.\n",
      "\n",
      "I am particularly excited about the opportunity to work at TechCorp because of the company's commitment to innovation and technology. I believe that my skills and experience align perfectly with the job requirements, and I am confident that I can make significant contributions to the team.\n",
      "\n",
      "Some of my notable achievements include developing a predictive model that increased sales by 15% for one of our clients, as well as creating a data visualization tool that improved communication between cross-functional teams. I am committed to staying up-to-date with industry trends and best practices in data science, and I am eager to apply my skills to solve complex problems and drive business growth at TechCorp.\n",
      "\n",
      "Thank you for considering my application. I look forward to discussing this opportunity further.\n",
      "\n",
      "Sincerely,\n",
      "John Doe\n",
      "+1234567890 | johndoe@example.com\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load LLaMA 2 model using Ollama\n",
    "llm = Ollama(model=\"llama2\", temperature=0.7)\n",
    "\n",
    "# Function to generate a recommendation letter\n",
    "def generate_recommendation_letter(cv_data, job_description):\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "    I need a cover letter based on their CV and the job description provided. The letter should emphasize the candidate's relevant skills, \n",
    "    experience, and achievements while tailoring the content to the requirements of the job. Below are the details:\n",
    "\n",
    "    CV:\n",
    "    Name: {cv_data['name']}\n",
    "    Email: {cv_data['contact']['email']}\n",
    "    Phone: {cv_data['contact']['phone']}\n",
    "\n",
    "    Education:\n",
    "    {', '.join(cv_data['education'])}\n",
    "\n",
    "    Experience:\n",
    "    {', '.join(cv_data['experience'])}\n",
    "\n",
    "    Skills:\n",
    "    {', '.join(cv_data['skills'])}\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the recommendation letter using LLaMA 2\n",
    "    recommendation_letter = llm(prompt)\n",
    "    return recommendation_letter\n",
    "\n",
    "# Generate the recommendation letter\n",
    "recommendation_letter = generate_recommendation_letter(cv_data, job_description)\n",
    "print(\"Generated Recommendation Letter:\\n\", recommendation_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recommendation_letter1.txt\", \"w\") as file:\n",
    "    file.write(recommendation_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f94bb7713d4d558d7a88fc966a67bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\olga_bassant_roadmap\\B2B_project\\-TalentForge\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LOQ\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3685547425f6428e83bead69c13fac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f294bf88c9444ba690a60674c116a01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbeb36c41f64a7593faf984b44780d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ee7177a19947919f34e431178d0aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c5e66ffda4ca283422b3d05c44578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0275d1bed90b4869b497c5e6062f395e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 9.2073974609375\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example generated text\n",
    "perplexity = calculate_perplexity(recommendation_letter)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Score: 34.46\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "# Calculate Flesch-Kincaid readability score\n",
    "def calculate_readability(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "# Example generated text\n",
    "readability = calculate_readability(recommendation_letter)\n",
    "print(\"Readability Score:\", readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

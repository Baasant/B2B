{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !D:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Scripts\\activate && pip install sentence-transformers\n",
    "!D:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Scripts\\activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4generator_extractor= Ollama(model=\"llama2\", temperature=0.7)\n",
    "# llm4generator_extractor= Ollama(model=\"deepseek-r1\", temperature=0.7)\n",
    "llm4eval= Ollama(model=\"deepseek-r1\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read job description from text file\n",
    "with open(\"Job Description1.txt\", \"r\") as file:\n",
    "    job_description = file.read()\n",
    "\n",
    "with open(\"cv.txt\", \"r\") as file:\n",
    "    cv_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4extract_data = f\"\"\"\n",
    "\n",
    "You are an intelligent assistant designed to extract structured data from a CV. Your task is to **analyze the entire CV** and extract **all relevant information**, ensuring that no key details are missed. If any information is unavailable, mark it as \"Not Available.\" The extracted information should be formatted **clearly and concisely in plain text.**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Data to Extract:**\n",
    "\n",
    "1. **Name**  \n",
    "   - Extract the **full name** of the individual.\n",
    "\n",
    "2. **Contact Information**  \n",
    "   - Email address  \n",
    "   - Phone number  \n",
    "\n",
    "3. **Education History**  \n",
    "   - Degree(s) earned (e.g., Bachelor of Science in Computer Science)  \n",
    "   - Institution(s) attended (e.g., University of XYZ)  \n",
    "   - Year(s) of graduation, if available  \n",
    "\n",
    "4. **Work Experience**  \n",
    "   Extract **all job experiences**, even if they are embedded within descriptions, projects, or skills sections. Ensure that each entry includes:  \n",
    "   - **Job Title** (e.g., Software Engineer)  \n",
    "   - **Company Name** (e.g., ABC Corp)  \n",
    "   - **Duration of Employment** (e.g., Jan 2020 â€“ Dec 2022), if available  \n",
    "   - **Job Responsibilities & Achievements** (if available)  \n",
    "\n",
    "   ðŸ”¹ **Include all experiences, even if they appear in different formats** (e.g., paragraphs, bullet points, or tables).  \n",
    "   ðŸ”¹ **If a duration is missing, extract at least the job title and company.**  \n",
    "\n",
    "5. **Skills**  \n",
    "   Extract **all mentioned skills**, including those found in job descriptions, project details, or other sections. Categorize them as follows:  \n",
    "   - **Programming Languages** (e.g., Python, C++, Java)  \n",
    "   - **Frameworks & Libraries** (e.g., TensorFlow, Flask, React)  \n",
    "   - **Tools & Technologies** (e.g., Docker, AWS, Git)  \n",
    "   - **Soft Skills** (e.g., Leadership, Communication, Problem-Solving)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Output Format:**  \n",
    "Name: [Full Name]\n",
    "\n",
    "Contact Information:\n",
    "Email: [Email Address]\n",
    "Phone: [Phone Number]\n",
    "\n",
    "Education History:\n",
    "\n",
    "[Degree], [Institution], [Year of Graduation]\n",
    "[Degree], [Institution], [Year of Graduation]\n",
    "Work Experience:\n",
    "\n",
    "[Job Title], [Company], [Duration]\n",
    "Responsibilities:\n",
    "\n",
    "[Responsibility 1]\n",
    "[Responsibility 2]\n",
    "[Job Title], [Company], [Duration]\n",
    "Responsibilities:\n",
    "\n",
    "[Responsibility 1]\n",
    "[Responsibility 2]\n",
    "Skills:\n",
    "Programming Languages:\n",
    "\n",
    "[Skill 1]\n",
    "[Skill 2]\n",
    "Frameworks & Libraries:\n",
    "\n",
    "[Skill 1]\n",
    "[Skill 2]\n",
    "Tools & Technologies:\n",
    "\n",
    "[Skill 1]\n",
    "[Skill 2]\n",
    "Soft Skills:\n",
    "\n",
    "[Skill 1]\n",
    "[Skill 2]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Instructions:**\n",
    "\n",
    "1. **Ensure Thorough Analysis**  \n",
    "   - Examine **all sections**, including headers, bullet points, and paragraphs.  \n",
    "   - Extract work experience **even if not explicitly labeled** as \"Work Experience.\"  \n",
    "\n",
    "2. **Handle Multiple Entries**  \n",
    "   - Include **all** education and work experience entries.  \n",
    "\n",
    "3. **Extract Implicit Skills**  \n",
    "   - Identify **skills hidden in job descriptions** and responsibilities.  \n",
    "\n",
    "4. **Mark Missing Information**  \n",
    "   - If any detail (e.g., graduation year) is unavailable, output **\"Not Available.\"**  \n",
    "\n",
    "5. **Ensure Accuracy & Formatting**  \n",
    "   - Structure the output **cleanly and consistently.**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example CV Text**  \n",
    "\n",
    "Name: John Smith\n",
    "\n",
    "Contact Information:\n",
    "Email: john.smith@example.com\n",
    "Phone: +1 555 123 4567\n",
    "\n",
    "Education History:\n",
    "\n",
    "Master of Science in Computer Science, Stanford University, 2021\n",
    "Bachelor of Science in Electrical Engineering, MIT, 2019\n",
    "Work Experience:\n",
    "\n",
    "Senior Software Engineer, Tech Innovators Inc., Jan 2022 â€“ Present\n",
    "Responsibilities:\n",
    "\n",
    "Led backend development for AI-driven products\n",
    "Improved system scalability by 40% using cloud technologies\n",
    "Software Engineer, CodeCraft LLC, Jun 2019 â€“ Dec 2021\n",
    "Responsibilities:\n",
    "\n",
    "Developed web applications using React and Flask\n",
    "Optimized database queries to enhance performance\n",
    "Skills:\n",
    "Programming Languages:\n",
    "\n",
    "Python\n",
    "Java\n",
    "JavaScript\n",
    "Frameworks & Libraries:\n",
    "\n",
    "Flask\n",
    "React\n",
    "TensorFlow\n",
    "Tools & Technologies:\n",
    "\n",
    "Docker\n",
    "AWS\n",
    "Git\n",
    "Soft Skills:\n",
    "\n",
    "Leadership\n",
    "Team Collaboration\n",
    "Problem-Solving\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Task:**  \n",
    "Now, extract the structured information from the following CV:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to run llm prompt to extract the data of the  cv file\n",
    "def extract_cv_info_with_llm(cv_text,prompt4extract_data):\n",
    "    # Define a prompt for the LLM to extract structured information\n",
    "    prompt4extract_data=prompt4extract_data+cv_text\n",
    "    # extract cv data\n",
    "    cv_data = llm4generator_extractor(prompt4extract_data)\n",
    "    return cv_data\n",
    "    \n",
    "with open(\"cv.txt\", \"r\") as file:\n",
    "    cv_text = file.read()\n",
    "\n",
    "cv_data = extract_cv_info_with_llm(cv_text,prompt4extract_data)\n",
    "print(\"Extracted CV Data:\", cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Prepare the prompt\n",
    "phase1_prompt4genrator = f\"\"\"\n",
    "I need a cover letter based on their CV and the job description provided. The letter should emphasize the candidate's relevant skills, \n",
    "experience, and achievements while tailoring the content to the requirements of the job. Below are the details:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a cover letter\n",
    "def generate_cover_letter(cv_data, job_description):\n",
    "    # concate all the info for the project\n",
    "    prompt = phase1_prompt4genrator+f\"\"\"\n",
    "   \n",
    "    CV:\n",
    "    {cv_data}\n",
    "    \n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    you must ensure that it concludes with the CV owner's name as extracted from the CV. The name should appear at the very end of the letter.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Generate the cover letter using LLaMA 2\n",
    "    cover_letter = llm4generator_extractor(prompt)\n",
    "    return cover_letter\n",
    "# generate the cover letter\n",
    "cover_letter = generate_cover_letter(cv_data, job_description)\n",
    "print(\"Generated cover Letter:\\n\", cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_letter\n",
    "with open(\"gnerate_cover_1.txt\", \"w\") as file:\n",
    "    file.write(cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the cover letter\n",
    "def evaluate_cover_letter(cv_data, job_description, cover_letter):\n",
    "    # Prepare the evaluation prompt\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Please evaluate the following cover letter based on the criteria listed below. For each point, provide a score from 1 (poor) to 5 (excellent), along with a brief explanation or examples supporting your score. Then, provide detailed feedback on the letter, including specific suggestions for improvement where necessary. Finally, include a summary stating whether the letter is **\"Approved\"** or **\"Needs Improvement.\"**\n",
    "\n",
    "\n",
    "    **Criteria for Evaluation:**  \n",
    "    1. **Tailored to the Role/Context:**  \n",
    "    - Does the letter address the specific job, role, or context clearly?  \n",
    "    - Is it customized to the recipient (e.g., mentioning the company, university, or field of interest)?  \n",
    "\n",
    "    2. **Strong Opening:**  \n",
    "    - Does the letter start with an engaging introduction?  \n",
    "    - Is the relationship between the writer and the subject of the letter clearly established?  \n",
    "\n",
    "    3. **Showcases Value:**  \n",
    "    - Does the letter effectively highlight the subject's key strengths, skills, or achievements?  \n",
    "    - Are these strengths relevant to the context or role in question?  \n",
    "\n",
    "    4. **Concrete Examples and Achievements:**  \n",
    "    - Are specific examples or achievements mentioned to back up claims about the subject?  \n",
    "    - Are these examples impactful and quantifiable (e.g., using metrics, measurable results)?  \n",
    "\n",
    "    5. **Well-Structured and Concise:**  \n",
    "    - Is the letter logically organized and easy to follow?  \n",
    "    - Is it concise, avoiding unnecessary information while covering all critical points?  \n",
    "\n",
    "    6. **Professional Tone and Authenticity:**  \n",
    "    - Does the tone remain professional throughout the letter?  \n",
    "    - Does the letter feel genuine and sincere in its praise?  \n",
    "\n",
    "    7. **Strong Closing:**  \n",
    "    - Does the letter end with a compelling summary and cover?  \n",
    "    - Is there a clear call to action or offer for further communication?  \n",
    "\n",
    "    8. **Error-Free and Polished:**  \n",
    "    - Is the letter free from grammatical, spelling, or formatting errors?  \n",
    "    - Does the formatting look clean and professional?  \n",
    "\n",
    "    ---\n",
    "\n",
    "    **Output Format:**  \n",
    "    1. **Score for Each Criterion (1â€“5)**: Provide a score and brief explanation for each criterion.  \n",
    "    2. **Detailed Feedback**:  \n",
    "    - Highlight specific strengths of the letter.  \n",
    "    - Identify areas needing improvement, with actionable suggestions for refinement.  \n",
    "    3. **Summary**:  \n",
    "    - Indicate whether the letter is **\"Approved\"** or **\"Needs Improvement.\"**  \n",
    "    - If **\"Needs Improvement,\"** explain what must be addressed to meet approval. \n",
    " \n",
    "\n",
    "   \n",
    "    ---\n",
    "Here are the needed data\n",
    "\n",
    "        CV:\n",
    "        {cv_data}\n",
    "        \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "        \n",
    "        cover Letter:\n",
    "        {cover_letter}\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # use the deepseek  model to evaluate the letter\n",
    "    feedback = llm4eval(evaluation_prompt)\n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_cover_letter(feedback,cv_data,job_description,current_letter):\n",
    "        refinement_prompt = f\"\"\"\n",
    "        The evaluator provided the following feedback on your previous cover letter:\n",
    "        {feedback}\n",
    "\n",
    "        Based on this feedback, refine the cover letter to improve its quality while addressing the issues highlighted.\n",
    "        Ensure it remains professional and well-aligned with the job description.\n",
    "\n",
    "        Here are the details for context:\n",
    "\n",
    "        **Candidate's CV**:\n",
    "        {cv_data}\n",
    "\n",
    "        **Job Description**:\n",
    "        {job_description}\n",
    "\n",
    "        **Current Cover Letter**:\n",
    "        {current_letter}\n",
    "\n",
    "        Generate a new and improved version of the cover letter.\n",
    "        \n",
    "        you must only add the new cover letter dont add the feedback or job Description\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the refined cover letter\n",
    "        current_letter = llm4generator_extractor(refinement_prompt)\n",
    "        return current_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_loop(cv_data, job_description, max_iterations=2):\n",
    "    current_letter = generate_cover_letter(cv_data, job_description)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}: Evaluating the letter...\\n\")\n",
    "        # get the feedback \n",
    "        feedback = evaluate_cover_letter(cv_data, job_description, current_letter)\n",
    "        \n",
    "        # Display feedback\n",
    "        # print(f\"Feedback from evaluator:\\n{feedback}\\n\")\n",
    "        \n",
    "        # Check if the letter is approved\n",
    "        if \"Approved\" in feedback:\n",
    "            print(\"The letter has been approved\")\n",
    "            return current_letter\n",
    "    \n",
    "        else:\n",
    "            print(\"Refining the cover letter based on feedback...\\n\") \n",
    "            print(\"****************************************************************\")\n",
    "            current_letter=refined_cover_letter(feedback,cv_data,job_description,current_letter)\n",
    "\n",
    "    print(\"Max iterations reached. Returning the latest version of the letter.\")\n",
    "    return current_letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_letter=feedback_loop(cv_data, job_description, max_iterations=2)\n",
    "with open(\"cover_letter4_atterations.txt\", \"w\") as file:\n",
    "    file.write(cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feedback loop to refine the cover letter\n",
    "# def feedback_loop(cv_data, job_description, max_iterations=2):\n",
    "#     current_letter = generate_cover_letter(cv_data, job_description)\n",
    "    \n",
    "#     for iteration in range(max_iterations):\n",
    "#         print(f\"Iteration {iteration + 1}: Evaluating the letter...\\n\")\n",
    "        \n",
    "#         # Get feedback from the evaluator\n",
    "#         feedback = evaluate_cover_letter(cv_data, job_description, current_letter)\n",
    "        \n",
    "#         # Display feedback\n",
    "#         print(f\"Feedback from evaluator:\\n{feedback}\\n\")\n",
    "        \n",
    "#         # Check if the letter is approved\n",
    "#         if \"Approved\" in feedback:\n",
    "#             print(\"The letter has been approved!\")\n",
    "#             return current_letter\n",
    "        \n",
    "#         else:\n",
    "#             print(\"reed modification...................\")\n",
    "#             # If not approved, refine the letter based on feedback\n",
    "#             refinement_prompt = f\"\"\"\n",
    "#             The evaluator provided the following feedback on your previous letter:\n",
    "#             {feedback}\n",
    "\n",
    "#             Based on this feedback, refine the cover letter to address the issues and improve its quality. \n",
    "#             Here are the original details for context:\n",
    "\n",
    "#             **Candidate's CV**:\n",
    "#             {cv_data}\n",
    "\n",
    "#             **Job Description**:\n",
    "#             {job_description}\n",
    "\n",
    "#             **Previous cover Letter**:\n",
    "#             {current_letter}\n",
    "\n",
    "#             Refine the letter while maintaining a professional tone and aligning it with the feedback.\n",
    "#             \"\"\"\n",
    "#             current_letter = llm4generator_extractor(refinement_prompt)\n",
    "\n",
    "#     print(\"Max iterations reached. Returning the latest version of the letter.\")\n",
    "#     return current_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_letter=feedback_loop(cv_data, job_description, max_iterations=2)\n",
    "# with open(\"cover_letter4all.txt\", \"w\") as file:\n",
    "#     file.write(cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cover_letter.txt\", \"w\") as file:\n",
    "    file.write(cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_feedback_fun(user_feedback,cv_data,job_description,current_letter):\n",
    "        refinement_prompt = f\"\"\"\n",
    "        The evaluator provided the following feedback on your previous cover letter:\n",
    "        {user_feedback}\n",
    "\n",
    "        Based on this feedback, refine the cover letter to improve its quality while addressing the issues highlighted.\n",
    "        Ensure it remains professional and well-aligned with the job description.\n",
    "\n",
    "        Here are the details for context:\n",
    "\n",
    "        **Candidate's CV**:\n",
    "        {cv_data}\n",
    "\n",
    "        **Job Description**:\n",
    "        {job_description}\n",
    "\n",
    "        **Current Cover Letter**:\n",
    "        {current_letter}\n",
    "\n",
    "        Generate a new and improved version of the cover letter.\n",
    "        \n",
    "        you must only add the new cover letter dont add the feedback or job Description\n",
    "        the cover letter must End with the candidateâ€™s name as extracted for CV.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the refined cover letter\n",
    "        refined_COV = llm4generator_extractor(refinement_prompt)\n",
    "        print(\"refined_COV\")\n",
    "        print(refined_COV)\n",
    "        print(\"*****************************************************\")\n",
    "        return refined_COV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feedback=\"\"\"\n",
    "please add Bassant Elsayed after Sincerely,\n",
    "\n",
    "\"\"\"\n",
    "final_COV=user_feedback_fun(user_feedback,cv_data,job_description,cover_letter)\n",
    "with open(\"final_cv.txt\", \"w\") as file:\n",
    "    file.write(final_COV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy nltk rake-nltk\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (3.8.4)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rake-nltk\n",
      "  Using cached rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\olga_bassant_roadmap\\b2b_project\\b2b\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: nltk, rake-nltk\n",
      "Successfully installed nltk-3.9.1 rake-nltk-1.0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "d:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Scripts\\python.exe: No module named python\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install spacy nltk rake-nltk\n",
    "!{sys.executable} -m python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cv.txt\", \"r\") as file:\n",
    "    cv_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "import spacy\n",
    "import nltk\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LOQ/nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\share\\\\nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LOQ\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(rake_phrases \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(spacy_phrases)))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Extract key phrases from CV and job description\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m cv_phrases \u001b[38;5;241m=\u001b[39m \u001b[43mextract_key_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m job_phrases \u001b[38;5;241m=\u001b[39m extract_key_phrases(job_description_text)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Merge results and remove duplicates\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 54\u001b[0m, in \u001b[0;36mextract_key_phrases\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_key_phrases\u001b[39m(text):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Extract using RAKE\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mrake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     rake_phrases \u001b[38;5;241m=\u001b[39m rake\u001b[38;5;241m.\u001b[39mget_ranked_phrases()\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Extract using spaCy\u001b[39;00m\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\rake_nltk\\rake.py:126\u001b[0m, in \u001b[0;36mRake.extract_keywords_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_keywords_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method to extract keywords from the text provided.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    :param text: Text to extract keywords from, provided as a string.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     sentences: List[Sentence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_text_to_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_keywords_from_sentences(sentences)\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\rake_nltk\\rake.py:180\u001b[0m, in \u001b[0;36mRake._tokenize_text_to_sentences\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_tokenize_text_to_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Sentence]:\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes the given text string into sentences using the configured\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    sentence tokenizer. Configuration uses `nltk.tokenize.sent_tokenize`\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    by default.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    :return: List of sentences as per the tokenizer used.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LOQ/nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\share\\\\nltk_data'\n    - 'd:\\\\olga_bassant_roadmap\\\\B2B_project\\\\B2B\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LOQ\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import spacy\n",
    "# import nltk\n",
    "\n",
    "# # Download NLTK stopwords (required for RAKE)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Initialize RAKE and spaCy\n",
    "rake = Rake()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Input 1: CV\n",
    "cv_text = \"\"\"\n",
    "Name: John Doe\n",
    "Email: johndoe@example.com\n",
    "Phone: +1234567890\n",
    "\n",
    "Education:\n",
    "- Bachelor of Science in Computer Science, XYZ University, 2015-2019\n",
    "- Master of Science in Data Science, ABC University, 2019-2021\n",
    "\n",
    "Experience:\n",
    "- Software Engineer at TechCorp, 2021-Present\n",
    "- Data Analyst at DataWorks, 2019-2021\n",
    "\n",
    "Skills:\n",
    "- Python\n",
    "- Machine Learning\n",
    "- SQL\n",
    "- Data Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Input 2: Job Description\n",
    "job_description_text = \"\"\"\n",
    "Machine Learning Engineer, Network Assurance Data Platform\n",
    "Rolle, Switzerland\n",
    "Please note that we have a hybrid approach to work and are looking for someone who can come into our offices in Rolle two days per week.\n",
    "Who We Are\n",
    "Cisco ThousandEyes is a Digital Experience Assurance platform that empowers organizations to deliver flawless digital experiences across every network â€“ even the ones they donâ€™t own. Powered by AI and an unmatched set of cloud, internet and enterprise network telemetry data, ThousandEyes enables IT teams to proactively detect, diagnose, and remediate issues â€“ before they impact end- user experiences.\n",
    "\n",
    "ThousandEyes is deeply integrated across the entire Cisco technology portfolio and beyond, helping customers deploy at scale while also delivering AI-powered assurance insights within Ciscoâ€™s leading Networking, Security, Collaboration, and Observability portfolios.\n",
    "\n",
    "About The Role\n",
    "As an AI Researcher on the Network Assurance Data Platform team, you will be responsible for building the next-generation routing engine for the Internet using Data Science, Machine Learning (ML), and Artificial Intelligence (AI). The size, diversity, and richness of our data are simply unmatched. The Internet is changing fast, and we are too. The objective of this confidential product is to adopt a highly novel and ambitious approach for optimizing networks to better serve applications.\n",
    "\n",
    "What Youâ€™ll Do\n",
    "You and your team will be designing and implementing large-scale machine learning pipelines. Our technology stack includes Python as well as a wide range of internal tools built on top of Apache Spark, TensorFlow, and other Big Data technologies.\n",
    "\"\"\"\n",
    "\n",
    "# Function to extract key phrases\n",
    "def extract_key_phrases(text):\n",
    "    # Extract using RAKE\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    rake_phrases = rake.get_ranked_phrases()\n",
    "    \n",
    "    # Extract using spaCy\n",
    "    doc = nlp(text)\n",
    "    spacy_phrases = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        spacy_phrases.add(chunk.text)\n",
    "    for ent in doc.ents:\n",
    "        spacy_phrases.add(ent.text)\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    return list(set(rake_phrases + list(spacy_phrases)))\n",
    "\n",
    "# Extract key phrases from CV and job description\n",
    "cv_phrases = extract_key_phrases(cv_text)\n",
    "job_phrases = extract_key_phrases(job_description_text)\n",
    "\n",
    "# Merge results and remove duplicates\n",
    "key_phrases = list(set(cv_phrases + job_phrases))\n",
    "\n",
    "# Filter out irrelevant phrases (optional)\n",
    "# irrelevant_phrases = [\"name\", \"email\", \"phone\", \"rolle\", \"switzerland\"]\n",
    "key_phrases = [phrase for phrase in key_phrases]\n",
    "# key_phrases = [phrase for phrase in key_phrases if phrase.lower() not in irrelevant_phrases]\n",
    "\n",
    "print(\"Key Phrases:\", key_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 7 stored elements and shape (1, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 9 stored elements and shape (1, 9)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 5)\t1\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 7 stored elements and shape (1, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 5)\t1\n",
      "Context Recall: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def extract_keywords(text):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    print(X)\n",
    "    return set(vectorizer.get_feature_names_out())\n",
    "\n",
    "def compute_recall(cv, job_desc, cover_letter):\n",
    "    # Extract unique keywords\n",
    "    cv_keywords = extract_keywords(cv)\n",
    "    jd_keywords = extract_keywords(job_desc)\n",
    "    cl_keywords = extract_keywords(cover_letter)\n",
    "\n",
    "    # Combine CV and Job Description keywords\n",
    "    all_keywords = cv_keywords.union(jd_keywords)\n",
    "\n",
    "    # Compute recall\n",
    "    recall = len(cl_keywords.intersection(all_keywords)) / len(all_keywords) if len(all_keywords) > 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Example inputs\n",
    "cv_text = \"Machine Learning Engineer with experience in Python, TensorFlow, and NLP.\"\n",
    "job_desc_text = \"Looking for an ML Engineer skilled in Python, NLP, and deploying AI models.\"\n",
    "cover_letter_text = \"I am an experienced Machine Learning Engineer with expertise in Python and NLP.\"\n",
    "\n",
    "# Compute context recall\n",
    "recall_score = compute_recall(cv_text, job_desc_text, cover_letter_text)\n",
    "print(f\"Context Recall: {recall_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cover_letter.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob Description1.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     job_desc_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread() \n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcover_letter.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m     cv_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\olga_bassant_roadmap\\B2B_project\\B2B\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cover_letter.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"cv.txt\", \"r\") as file:\n",
    "    cv_text = file.read()\n",
    "with open(\"job Description1.txt\", \"r\") as file:\n",
    "    job_desc_text = file.read() \n",
    "with open(\"cover_letter.txt\", \"r\") as file:\n",
    "    cv_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D:\\olga_bassant_roadmap\\B2B_project\\B2B\\app\\llm4allV1.1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Context Recall: 0.49\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def compute_bert_recall(cv, job_desc, cover_letter):\n",
    "    # Load pre-trained BERT model for embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Get sentence embeddings\n",
    "    cv_embedding = model.encode(cv)\n",
    "    jd_embedding = model.encode(job_desc)\n",
    "    cl_embedding = model.encode(cover_letter)\n",
    "\n",
    "    # Compute combined embedding for CV + JD\n",
    "    combined_cv_jd = np.mean([cv_embedding, jd_embedding], axis=0)\n",
    "\n",
    "    # Compute cosine similarity between Cover Letter and (CV + JD)\n",
    "    similarity = cosine_similarity([combined_cv_jd], [cl_embedding])[0][0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Compute BERT-based recall\n",
    "# Example inputs\n",
    "# cv_text = \"Machine Learning Engineer with experience in Python, TensorFlow, and NLP.\"\n",
    "# job_desc_text = \"Looking for an ML Engineer skilled in Python, NLP, and deploying AI models.\"\n",
    "# cover_letter_text = \"I am an experienced Machine Learning Engineer with expertise in Python and NLP.\"\n",
    "\n",
    "\n",
    "bert_recall = compute_bert_recall(cv_text, job_desc_text, cover_letter_text)\n",
    "print(f\"BERT Context Recall: {bert_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak Sentences in Cover Letter (low relevance to CV/Job Description):\n",
      "\n",
      " As a seasoned Machine Learning Engineer with a background in Electronics and Electrical Communication Engineering, I bring a unique blend of technical expertise and problem-solving abilities to the table (Similarity: 0.57)\n",
      "************************************************************************************\n",
      " Additionally, I designed intelligent systems and tools for RF planning and optimization, enhancing decision-making processes (Similarity: 0.54)\n",
      "************************************************************************************\n",
      " These experiences have honed my ability to develop innovative solutions that drive business success.\n",
      "\n",
      "My education in Electronics and Electrical Communication Engineering has provided me with a solid foundation in programming & frameworks (Python, PyTorch, TensorFlow, Hugging Face Transformers), NLP & LLM expertise (fine-tuning Transformer-based models, prompt engineering, text classification, sentiment analysis), and AI & ML expertise (sequence modeling, transfer learning, few-shot and zero-shot learning, model deployment) (Similarity: 0.50)\n",
      "************************************************************************************\n",
      " As a team, we will be designing and implementing large-scale machine learning pipelines using Python as well as a wide range of internal tools built on top of Apache Spark, TensorFlow, and other Big Data technologies (Similarity: 0.50)\n",
      "************************************************************************************\n",
      " I bring a deep understanding of the intricacies inherent in machine learning systems and the ability to transform abstract concepts into robust, efficient, and scalable solutions.\n",
      "\n",
      "Cisco values diversity and inclusivity, and I am eager to contribute to a workplace culture that fosters collaboration, creativity, and innovation (Similarity: 0.51)\n",
      "************************************************************************************\n",
      " As an Affirmative Action and Equal Opportunity Employer, Cisco considers qualified applicants with arrest and conviction records for employment.\n",
      "\n",
      "Thank you for considering my application (Similarity: 0.26)\n",
      "************************************************************************************\n",
      " I look forward to the opportunity to discuss how my skills and experience align with your requirements.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name] (Similarity: 0.30)\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Lightweight and effective\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "# Load CV, Job Description, and Cover Letter from files\n",
    "cv_text = read_text_file(\"cv.txt\")\n",
    "job_desc_text = read_text_file(\"job Description1.txt\")\n",
    "cover_letter_text = read_text_file(\"cover_letter.txt\")\n",
    "\n",
    "# Split cover letter into sentences\n",
    "cover_letter_sentences = cover_letter_text.split(\". \")  # Splitting by sentence\n",
    "\n",
    "# Combine CV and Job Description for comparison\n",
    "cv_jd_texts = [cv_text, job_desc_text]\n",
    "cv_jd_embeddings = model.encode(cv_jd_texts, convert_to_tensor=True)\n",
    "\n",
    "# Encode cover letter sentences\n",
    "cover_letter_embeddings = model.encode(cover_letter_sentences, convert_to_tensor=True)\n",
    "\n",
    "# Identify weak sentences\n",
    "weak_sentences = []\n",
    "\n",
    "for i, sentence in enumerate(cover_letter_sentences):\n",
    "    similarity_scores = util.pytorch_cos_sim(cover_letter_embeddings[i], cv_jd_embeddings)\n",
    "    max_similarity = similarity_scores.max().item()  # Get highest similarity score\n",
    "\n",
    "    # If similarity is below a threshold (e.g., 0.6), consider it weak\n",
    "    if max_similarity < 0.6:\n",
    "        weak_sentences.append((sentence, max_similarity))\n",
    "\n",
    "# Print weak sentences\n",
    "if weak_sentences:\n",
    "    print(\"Weak Sentences in Cover Letter (low relevance to CV/Job Description):\\n\")\n",
    "    for sent, score in weak_sentences:\n",
    "        print(f\" {sent} (Similarity: {score:.2f})\")\n",
    "        print(\"************************************************************************************\")\n",
    "else:\n",
    "    print(\" No weak sentences detected! Your cover letter is well-aligned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
